{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four Vs of Big Data\n",
    "- Volume refers to the size of data (e.g., terabytes of product information). For instance, a year's worth of stock market transactions is a large amount of data.0\n",
    "- Velocity pertains to how quickly data comes in (customers across the world purchasing every second). As an example, McDonald's restaurants are worldwide with customers buying food at a constant rate, so the data comes in fast.\n",
    "- Variety relates to different forms of data (e.g., user account information, product details, etc.). Consider the breadth of Netflix user information, videos, photos for thumbnails, and so forth.\n",
    "- Veracity concerns the uncertainty of data (e.g., reviews might not be real and could come from bots). As an example, Netflix would want to verify whether users are actively watching the shows, falling asleep, or just playing them in the background.\n",
    "\n",
    "### Big Data Problems\n",
    "The four Vs of big data will help you determine when to migrate from regular data to big data solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HaDoop\n",
    "\n",
    "Apache Hadoop (Hadoop) is one of the most popular open source frameworks, with numerous technologies for big data. Google developed Hadoop to process large amounts of data by splitting data across a distributed file system\n",
    "\n",
    "We'll start with the three main components of Hadoop:\n",
    "\n",
    "- Hadoop Distributed File System (HDFS) is a file system used to store data across server clusters (groups of computers). It is scalable (which means it handles influxes of data), fault-tolerant (handles hardware failure), and distributed (spread across multiple servers connected by a common core).\n",
    "- MapReduce is a programming model and processing technique for big data. MapReduce enables processing the large amount of data spread across the cluster in the HDFS by performing the same task for each file system.\n",
    "- Yet Another Resource Negotiator (YARN) manages and allocates resources across the clusters and assigns tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce Process\n",
    "\n",
    "MapReduce is used as a means for distributing and processing data on your cluster. MapReduce is built on the process of mapping—the process of assigning the same job to each of the computers—and reducing, which is when you come back together to combine the results. Mapping - processing input and producing small chunks of data across each computer- and reducing, which is when you come back together to combine the results.\n",
    "\n",
    "\n",
    "During the mapping stage the map function, which is the function applied to each computer, takes a small piece of the input and then converts the data into key-value pairs, with key identifiers and associated values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python has a library called mrjob, which stands for \"MapReduce job\" and\n",
    "# it can help us practice MapReduce outside of the Hadoop ecosystem.\n",
    "    # pip install MRJob\n",
    "from mrjob.job import MRJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [options] [input files]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9003 --control=9001 --hb=9000 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"7b718a54-566c-40f8-9aac-c5796763b999\" --shell=9002 --transport=\"tcp\" --iopub=9004\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# Create a class called Bacon_count, which inherits, or takes properties, from the MRJob class. We create this class to be called to run the full MapReduce job withMRJob:\n",
    "class Bacon_count(MRJob):    \n",
    "    # Next, create a mapper()function that will take (self, _, line) as parameters. \n",
    "    # The mapper() function will assign the input to key-value pairs:\n",
    "    # The second parameter (here using an underscore (_), explained next) allows methods to be mapped together. \n",
    "    # Since we are not chaining anything together, we use the Python convention of an underscore to indicate that we won’t use \n",
    "    # this parameter. The (line) parameter will be the line of text taken from the raw input file.\n",
    "    def mapper(self, _, line):\n",
    "        \n",
    "#The function will loop through each word in the line of text, as described below:\n",
    "# 1) Call the split() method on each line to break the text into a list of words.\n",
    "# 2) Each word will convert to lowercase.\n",
    "# 3) If the words match the search word \"bacon,\" a key-value pair will show as yield \"bacon\", 1.\n",
    "# 4) When you call a function with yield it returns what is called a generator object. A generator is an iterator like a list, \n",
    "# however unlike a list the contents are not stored in memory, useful for large files. \n",
    "# When yield is called the function is suspended and returns a value. \n",
    "# A generator won't return another value until next() is called, which is something that mrJobs \n",
    "# calls a number of times till it is done. So, for a yield, each time the word \"bacon\" appears, \n",
    "# mrJobs returns “bacon”, 1. If \"bacon\" appears three times, then an output of “bacon”, 1 would be produced three times.\n",
    "        for word in line.split():\n",
    "            if word.lower() == \"bacon\":\n",
    "                yield \"bacon\", 1 \n",
    "                \n",
    "# There's a shuffle step that occurs after the mapper. There is no code written for this step, \n",
    "# and it occurs because the class inherits from the mrjob library. This shuffle step organizes the \n",
    "# key-value pairs so that there's only one key for each unique key, and combines the values into a list.\n",
    "\n",
    "# The reducer function might not look like it's doing as much as the mapper function, but it's just as important. The reducer function takes three parameters: self, key, and values:\n",
    "# 1) The self parameter is used in Python to represent the instance of the class.\n",
    "# 2) The key parameter represents the key of the key-value pair created in the mapper function. In this example, the key is \"bacon.\"\n",
    "# 3) The values parameter is a list of values created in the mapper function. We want to sum all of these values. Recall that from the mapper function the yield was used to produce multiple outputs. With the reducer we'll produce the key and sum of all the values assigned with it:\n",
    "    def reducer(self, key, values):\n",
    "        yield key, sum(values)\n",
    "        \n",
    "# The final bit of code, shown below, is conventional Python code for running the program: \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   Bacon_count.run() \n",
    "\n",
    "# You might have noticed that nowhere in the code is a file imported or opened. The mrjob library works by reading in a file passed to it in the terminal.   \n",
    "        \n",
    "# run code in terminal:\n",
    "    # python Module_Practice.py input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features of Spark\n",
    "\n",
    "Hadoop is an ecosystem for handling big data. Expect to spend significant time configuring multiple servers or computers, as well as researching which technology can best deliver your big data solution. With the growing interest in big data and the ease of access to cloud technology, which we'll cover later, Hadoop is no longer required. New technologies allow more flexibility in data processing. One of these technologies is Spark.\n",
    "\n",
    "Apache Spark (Spark) is a unified analytics engine for large-scale data processing. Spark lets you write applications in code that can run on Hadoop. However, Spark doesn't have to run on Hadoop, as it can run in stand-alone mode or in the cloud. Spark can be 100 times faster than Hadoop. Just like Hadoop's MapReduce, Spark works with data spread across a cluster, or a group of computers that work together.\n",
    "\n",
    "Spark uses in-memory computation instead of a disk-based solution, which means it doesn't need to talk to the HDFS each time and can retain as much as HDFS can in-memory. Spark uses lazy evaluation, which delays the evaluation of an expression or command until the value is needed.\n",
    "\n",
    "For example, when you direct Spark to count all the product reviews and then group them by star rating, Spark is ready to start, but you'll need to initiate the task—at this point, no counting or grouping has been done, only the instructions have been given. Once you give the go-ahead, Spark will then count and group the reviews all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Architecture \n",
    "\n",
    "The Spark architecture includes the driver, executors, and the cluster manager:\n",
    "\n",
    "- The driver is the heart of the application. It is responsible for maintaining the application information; responding to the code or input; and analyzing, distributing, and scheduling work to the executors.\n",
    "- The executors perform the code assigned by the driver and then report the state of the computation to the driver.\n",
    "- The cluster manager controls the driver and executors and allocates resources to the machines on the Spark applications. The cluster manager is an external service for acquiring resources on the cluster. Spark can either use it's own standalone cluster manager that comes standard with Spark or another application (e.g., Apache Mesos, Hadoop YARN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Parallelism\n",
    "\n",
    "Parallelism in Spark entails running work through a cluster (group of computers) concurrently, instead of performing all work on a single computer. Recall when you and Jennifer were each counting video game reviews in two separate datasets, and you were working parallel to each other. You did not need to wait for Jennifer to do anything before you could start counting your reviews. This is exactly what Spark is doing.\n",
    "\n",
    "Data is broken into partitions, meaning a chunk of your data will sit on a physical machine in your cluster. If your Spark application has three machines, each one of those machines will have a piece of your original dataset. For example, a dataset with 1,000 rows of data might have 334 rows on one machine, 333 on a second, and 333 on a third.\n",
    "\n",
    "Note the following guidance when running code in parallel: If there is only one partition but many executors, the parallelism is only one. One machine can only work with one executor. The following image visualizes this concept.\n",
    "\n",
    "Each executor needs to work on each partition for perfect parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "Natural language processing (NLP) is a growing field of study that combines linguistics and computer science for computers to understand written, spoken, and typed natural language. NLP is the process of converting normal language to a machine readable format, which allows a computer to analyze text as if it were numerical data.\n",
    "\n",
    "While NLP has a wide variety of use cases, and the field is rapidly growing, there are a few use cases that are particularly interesting:\n",
    "\n",
    "- Analyzing legal documents: NLP can be used to analyze many types of legal documents. This can improve the outcome of a given case, as lawyers and staff can find critical information quickly.\n",
    "- U.S. Securities and Exchange Commission (SEC) filings: NLP is used to analyze SEC filings for various businesses. Companies use NLP to analyze filings for real-time business intelligence.\n",
    "- Chatbots: Chatbots are one of the most popular use cases. Chatbots can be used for selling products, customer support, and even medical help.\n",
    "\n",
    "At this point, you might ask how this relates to big data. Due to the massive amounts of text data needed to drive insights, we'll have to learn how to manage that data. There are a number of important use cases to delve into:\n",
    "\n",
    "- Classifying text: For many of the aforementioned use cases to work, a computer must know how to classify a given piece of text. Classification can mean a few different things in NLP. You can have classification of specific words, even specifying what the part of speech is. You can also classify what the text is as a whole.\n",
    "- Extracting information: Many NLP tasks require the ability to retrieve specific pieces of information from a given document. Think of the case where we are extracting data from law documents. You might want to extract certain aspects of that document to present good cases.\n",
    "- Summarizing a document: Summarization is a key aspect of NLP. It helps solve quite a few different problems. You can essentially create a model that summarizes a given document. This can be helpful to understand the high-level details of law documents, articles, and much more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context of Natural Language\n",
    "\n",
    "Natural language can be complicated because the way it's written is not always how it is intended. Therefore, you might need the full context to understand the meaning.\n",
    "\n",
    "Sarcasm is a great example. Say you had a bad experience at a restaurant. Your friend asks if you liked your meal and you reply \"Oh, yeah, the food was amazing if you like dry, bland food.\" A friend familiar with your humor would understand your true intentions behind the quip. However, a straight reading, without detecting sarcasm, would give the impression you prefer dry, bland food.\n",
    "\n",
    "Another challenge is interpreting the tone behind the text. For instance, snidely remarking \"Great\" and enthusiastically exclaiming \"Great!\" reveal two distinct tones but, in text, it is the same word.\n",
    "\n",
    "These are just two examples of the complexity of dealing with natural language. In the next section, we'll show how a computer does its best to interpret language.\n",
    "\n",
    "### Tokenization\n",
    "Tokenization is the concept of splitting a document or sentence into small subsets of data that can be analyzed. It's essentially the building block of most NLP use cases. Tokenization can be performed by word or sentence.\n",
    "\n",
    "To tokenize by word, you take a given sentence or document and split it into a list of words, with each sentence or word representing a token. A sentence tokenized by word would look like the following:\n",
    "\n",
    "Original sentence: I am enjoying learning about NLP.\n",
    "\n",
    "Tokenized by word: ['I', 'am', 'enjoying', 'learning', 'about', 'NLP', '.']\n",
    "\n",
    "To tokenize by sentence, you would provide a document with at least one sentence. Ideally, you would use more than one sentence, as tokenization splits a document up by sentences. Here's an example of a two-sentence document and how it would be split up. Notice how each sentence is a separate string:\n",
    "\n",
    "Original sentence: There is a concept in NLP called tokenization. There are two types of tokenization: word and sentence.\n",
    "\n",
    "Tokenized by sentence: ['There is a concept in NLP called tokenization.', 'There are two types of tokenization: word and sentence.']\n",
    "\n",
    "You can tokenize using NLTK or spaCy libraries, which we'll cover later.\n",
    "\n",
    "### Normalization\n",
    "Normalization is the concept of taking misspelled words and converting them into their original form. This is another building block in NLP in that it helps get the text to a readable form and allows us to create other use cases on top of it. Essentially, it makes it easier for us to create NLP programs, and it improves the output of those programs. There are numerous ways to accomplish this, but we'll focus on just two practices: stemming and lemmatization. Stemming and lemmatization are similar in that they both remove the suffix from a word, but there are some differences in how smooth or rough the cutoff tends to be:\n",
    "- Stemming removes the suffix from a word and reduces it to its original form. This serves as a “rough” cut off the end of the word. An example of stemming might be to reduce “horses” to “horse” and “ponies” to “poni.” As seen here, the truncated form is not always a real word\n",
    "- Lemmatization removes the suffix from a word and reduces it to its original form. Lemmatization tends to be a “smoother” cut off the end of the word. It tries to return to the original root word. In contrast to stemming, lemmatization always returns a real word. For example, the word “am” might be lemmatized to “be.” While stemming is a blunt instrument that follows abstract rules regardless of real world usage, lemmatization performs a similar process but reduces words to their root. Lemmatization accomplishes this by using a lexicon (a specialized dictionary) of words and their variant forms.\n",
    "\n",
    "### Part-of-Speech Tagging\n",
    "When researching or practicing NLP, you'll often hear of part-of-speech tagging (PoS), which can be helpful for a variety of different models in NLP. PoS tagging is the concept of finding each word's part of speech in a given document, as the example below illustrates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('enjoy', 'VBP'), ('biking', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('trails', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "text = word_tokenize(\"I enjoy biking on the trails\")\n",
    "output = nltk.pos_tag(text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('know', 'VB'), ('if', 'IN'), ('your', 'PRP$'), ('mind', 'NN'), ('is', 'VBZ'), ('messed', 'VBN'), ('up', 'RP'), ('from', 'IN'), ('all', 'PDT'), ('that', 'DT'), ('chocolate', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "new_text = word_tokenize(\"I don't know if your mind is messed up from all that chocolate\")\n",
    "output = nltk.pos_tag(new_text)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Generation\n",
    "Natural language generation is a growing field in NLP that entails writing code in such a way that it will generate new text. Popular examples include chatbots, automated custom reports, and custom webpage content. In this module, we won't write code to generate new text.\n",
    "\n",
    "Chatbots are easy to create and so are increasingly used in data science, especially NLP. Most of the technology already exists to create meaningful content from natural language generation.\n",
    "\n",
    "### Bag-of-Words\n",
    "When we're building datasets for NLP, we need to consider how our program will interact with the text document we provide. If we create a bag-of-words (BoW) model (i.e., the most frequent words), we can build models from that.\n",
    "\n",
    "The basic idea behind this model is this: We have a document of words, but we don't care about the order of the words. We can count these words and create models based on how frequently they appear.\n",
    "\n",
    "### n-gram\n",
    "In NLP, there is an n-gram method, which is a sequence of items from a given text. With n-gram, you create groupings of items from the text of size 1 or more. The following n-grams are common:\n",
    "\n",
    "Unigram is an n-gram of size 1.\n",
    "Bigram is an n-gram of size 2.\n",
    "Trigram is an n-gram of size 3.\n",
    "For example, a unigram for the sentence “I like pizzas” would be “I,” “like,” and “pizzas.” Its bigram would break up the sentence into sequential two-word chunks: “I like” and “like pizzas.” The trigram would be “I like pizzas.”\n",
    "\n",
    "Of course, for longer sentences, you can always use larger groupings than just size 3, such as “four-gram,” “five-gram,” and so on.\n",
    "\n",
    "N-grams can be used for a variety of NLP tasks, but most involve text mining or extraction. You can also use n-grams for spellcheck and text summarization.\n",
    "\n",
    "### Text Similarity\n",
    "Another popular use case for NLP is determining document or sentence similarity. These are important use cases, because they can tell us a lot about a document and its contents. There are a number of ways to do text similarity, with varying levels of difficulty. Many of the technologies that we'll discuss have the ability to compare documents against one another.\n",
    "\n",
    "### NLP Analyses\n",
    "There are three types of NLP analyses:\n",
    "\n",
    "- Syntactic analysis is essentially checking the dictionary definition of each element of a sentence or document. In this type of analysis, we don't care about the words that come before or after the word in question—we just care about the given word.\n",
    "- Sentiment analysis pertains to what the text means. Is it positive, negative, or neutral? You can come up with a score of how positive or negative the text is using NLP.\n",
    "- Semantic analysis entails extracting the meaning of the text. You want to analyze the meaning of each word, and then relate that to the meaning of the text as a whole.\n",
    "\n",
    "### Named-Entity Recognition\n",
    "In NLP, named-entity recognition (NER) is the concept of taking a document and finding all of the important terms therein. By \"important,\" we mean names of places and people, government organizations, and so forth. Many names are already recognized, but you can always add more names to the list of recognized entities, as necessary.\n",
    "\n",
    "You train a model on data labeled with important entities so that the model can better distinguish which entities should be labeled in a different dataset.\n",
    "\n",
    "## NLP Pipeline\n",
    "NLP is complicated. To manage it, you must build an NLP pipeline, a process breaking NLP down into a series of smaller, less complex tasks. Below we'll provide a high overview of this process, and in the next section, we'll dive deeper with the code.\n",
    "\n",
    "Each step of the NLP pipeline involves a separate task. The output data from one step, in turn, becomes the input data for the next step, with an opportunity to evaluate and refine each task, if needed. A basic NLP pipeline follows:\n",
    "1. Raw Text: Start with the raw data.\n",
    "2. Tokenization: Separate the words from paragraphs, to sentences, to individual words.\n",
    "3. Stop Words Filtering: Remove common words like \"a\" and \"the\" that add no real value to what we are looking to analyze.\n",
    "4. Term Frequency-Inverse Document Frequency (TF-IDF): Statistically rank the words by importance compared to the rest of the words in the text. This is also when the words are converted from text to numbers.\n",
    "5. Machine Learning: Put everything together and run through the machine learning model to produce an output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP Pipeline"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "249e5e7a09ecfff04bf2f3c245eb333db89e051ef4ebbe19a6a0f6f6e5d0b19d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('PythonData': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
